{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4_BCo0mvIkx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Jc2nmNtr-T"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yga4ptUevBIq"
      },
      "source": [
        "## 1. Paramètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtaFv3oxt1u1"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATASET_PATH = \"dataset_path\" #insérer le chemin d'accès au dataset'\n",
        "TRAIN_DIR = os.path.join(DATASET_PATH, \"train\")\n",
        "AUG_TRAIN_DIR = os.path.join(DATASET_PATH, \"train_augmented\")\n",
        "VAL_DIR = os.path.join(DATASET_PATH, \"val\")\n",
        "TEST_DIR = os.path.join(DATASET_PATH, \"test\")\n",
        "AUG_PER_IMAGE = 10  # beaucoup plus d’images par image originale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5XkSGJvQ2R"
      },
      "source": [
        "## 2. Fonction d’augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZRvBeY7t_up"
      },
      "outputs": [],
      "source": [
        "def apply_random_augmentation(image):\n",
        "    rows, cols = image.shape\n",
        "\n",
        "    # Random rotation\n",
        "    angle = random.uniform(-45, 45)\n",
        "    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
        "    image = cv2.warpAffine(image, M, (cols, rows), borderValue=255)\n",
        "\n",
        "    # Random scaling\n",
        "    scale = random.uniform(0.8, 1.2)\n",
        "    image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
        "\n",
        "    # Center crop or pad\n",
        "    if image.shape[0] > rows:\n",
        "        start = (image.shape[0] - rows) // 2\n",
        "        image = image[start:start+rows, start:start+cols]\n",
        "    else:\n",
        "        image = cv2.copyMakeBorder(image, 0, rows - image.shape[0], 0, cols - image.shape[1], cv2.BORDER_CONSTANT, value=255)\n",
        "\n",
        "    # Translation\n",
        "    dx = random.randint(-10, 10)\n",
        "    dy = random.randint(-10, 10)\n",
        "    M_trans = np.float32([[1, 0, dx], [0, 1, dy]])\n",
        "    image = cv2.warpAffine(image, M_trans, (cols, rows), borderValue=255)\n",
        "\n",
        "    # Flip\n",
        "    if random.random() < 0.5:\n",
        "        image = cv2.flip(image, 1)  # horizontal\n",
        "\n",
        "    # Add noise\n",
        "    noise = np.random.normal(0, 10, (rows, cols)).astype(np.uint8)\n",
        "    image = cv2.add(image, noise)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZiLptbZvVlO"
      },
      "source": [
        "## 3. Génération du dossier train_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-2rAsYEuLY4"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_data(input_dir, output_dir, augmentations_per_image):\n",
        "    for label in os.listdir(input_dir):\n",
        "        input_class_path = os.path.join(input_dir, label)\n",
        "        output_class_path = os.path.join(output_dir, label)\n",
        "        os.makedirs(output_class_path, exist_ok=True)\n",
        "\n",
        "        for img_name in os.listdir(input_class_path):\n",
        "            img_path = os.path.join(input_class_path, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Sauvegarder l’original\n",
        "            cv2.imwrite(os.path.join(output_class_path, f\"orig_{img_name}\"), img)\n",
        "\n",
        "            for i in range(augmentations_per_image):\n",
        "                aug_img = apply_random_augmentation(img)\n",
        "                aug_name = f\"{os.path.splitext(img_name)[0]}_aug{i}.png\"\n",
        "                cv2.imwrite(os.path.join(output_class_path, aug_name), aug_img)\n",
        "\n",
        "    print(\"Augmentation terminée.\")\n",
        "\n",
        "# Générer les données augmentées si elles n'existent pas encore\n",
        "if not os.path.exists(AUG_TRAIN_DIR):\n",
        "    print(\"Création des données augmentées...\")\n",
        "    generate_augmented_data(TRAIN_DIR, AUG_TRAIN_DIR, AUG_PER_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smDwjUypvbZ4"
      },
      "source": [
        "## 4. Extraction de caractéristiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3FEv5pfuQeB"
      },
      "outputs": [],
      "source": [
        "def extract_features(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        return [0]*6\n",
        "\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "    area = cv2.contourArea(contour)\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "    circularity = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    aspect_ratio = float(w) / h if h > 0 else 0\n",
        "    hull = cv2.convexHull(contour)\n",
        "    hull_area = cv2.contourArea(hull)\n",
        "    solidity = float(area) / hull_area if hull_area > 0 else 0\n",
        "    epsilon = 0.02 * cv2.arcLength(contour, True)\n",
        "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "    num_vertices = len(approx)\n",
        "\n",
        "    return [area, perimeter, circularity, aspect_ratio, solidity, num_vertices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfoaK6-ivey6"
      },
      "source": [
        "## 5. Chargement du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahuWWfCouTZb"
      },
      "outputs": [],
      "source": [
        "def load_dataset(folder_path):\n",
        "    X, y = [], []\n",
        "    for label in os.listdir(folder_path):\n",
        "        class_dir = os.path.join(folder_path, label)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            features = extract_features(img_path)\n",
        "            X.append(features)\n",
        "            y.append(label)\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKad4qXQv1oK"
      },
      "source": [
        "## 6. Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO852hwkuXSw"
      },
      "outputs": [],
      "source": [
        "print(\"Chargement des données augmentées...\")\n",
        "X_train, y_train = load_dataset(AUG_TRAIN_DIR)\n",
        "X_val, y_val = load_dataset(VAL_DIR)\n",
        "X_test, y_test = load_dataset(TEST_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ek0Puogv6p5"
      },
      "source": [
        "## 7. Encodage des labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJHTuHxhuZl5"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "y_train_enc = encoder.fit_transform(y_train)\n",
        "y_val_enc = encoder.transform(y_val)\n",
        "y_test_enc = encoder.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D4efD4LwCJv"
      },
      "source": [
        "##  8. Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DYQIzU8uewj"
      },
      "outputs": [],
      "source": [
        "print(\"Entraînement du modèle...\")\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKamrmWOwGn6"
      },
      "source": [
        "## 9. Évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZo2SM4lujQ3"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Évaluation sur validation ---\")\n",
        "val_pred = clf.predict(X_val)\n",
        "print(classification_report(y_val_enc, val_pred, target_names=encoder.classes_))\n",
        "\n",
        "print(\"\\n--- Évaluation sur test ---\")\n",
        "test_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test_enc, test_pred, target_names=encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00oPvrPwNTO"
      },
      "source": [
        "## 10. Matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LECO2MeGuoH0"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test_enc,\n",
        "    test_pred,\n",
        "    display_labels=encoder.classes_,\n",
        "    xticks_rotation=45,\n",
        "    cmap='Blues'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIuLfsa2weED"
      },
      "source": [
        "## 11. Matrice de coût personnalisée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAoP-YQzuuyC"
      },
      "outputs": [],
      "source": [
        "# L'ordre des classes doit correspond exactement à celui des classes encodées\n",
        "shape_order = list(encoder.classes_)\n",
        "cost_dict = {\n",
        "    'circle':           [0, 7, 8, 8, 8, 8, 6, 9],\n",
        "    'kite':             [7, 0, 4, 5, 3, 4, 5, 6],\n",
        "    'parallelogram':    [8, 4, 0, 2, 3, 2, 3, 7],\n",
        "    'rectangle':        [8, 5, 2, 0, 3, 1, 3, 6],\n",
        "    'rhombus':          [8, 3, 3, 3, 0, 1, 4, 6],\n",
        "    'square':           [8, 4, 2, 1, 1, 0, 3, 6],\n",
        "    'trapezoid':        [6, 5, 3, 3, 4, 3, 0, 5],\n",
        "    'triangle':         [9, 6, 7, 6, 6, 6, 5, 0]\n",
        "}\n",
        "\n",
        "# Créer la matrice de coût en respectant l'ordre encodé\n",
        "cost_matrix = np.array([cost_dict[cls] for cls in shape_order])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYL0Ue0Iu_Pd"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
